{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dancankimutai/basic-ml-course/blob/Neural-Network/07_Neural_Network/Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 7: Neural Network"
      ],
      "metadata": {
        "id": "tepFZ8MaX6RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: To run this notebook. You should change runtime type to GPU by: go to Runtime -> Change runtime type -> GPU"
      ],
      "metadata": {
        "id": "RWPSbcDu3Ef2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In lecture 7's video, we have shown you how to use Neural Network to classify handwritten digits on the MNIST dataset. In this notebook, we will go into the details. We also discover components that we have not talked about in the video."
      ],
      "metadata": {
        "id": "vyE5Obn6PGvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import some libraries and packages"
      ],
      "metadata": {
        "id": "wUrX5_l-X9KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4tB_dVjrC3b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "L7zxH2HHYNI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transforms.ToTensor())\n",
        "testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "73RIk4mSC-Jn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check an image and its label"
      ],
      "metadata": {
        "id": "f5404CDSZotp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = trainset[20]\n",
        "print('Label: ', label)\n",
        "print('Image shape: ', img.shape)\n",
        "plt.imshow(img[0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "DLqfRHE7FVnF",
        "outputId": "0e586ecf-2e83-4066-cbf1-065c647b13f8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  4\n",
            "Image shape:  torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7bbb562070>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch provides DataLoader to efficiently load data from dataset for both training and testing. For more details about the configurations, you can refer to https://pytorch.org/docs/stable/data.html"
      ],
      "metadata": {
        "id": "YxJduKT8fZ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "93XeSyr-G_si"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Neural Network has layers. To create a layer you can use `nn.Linear`. Let examine one layer.\n",
        "\n",
        "Here we create a layer which has 4 neurons and 2 inputs. Matrix of weight ($W$) has shape of 4x2, $W \\in \\mathbb{R}^{4*2}$. The bias vector of neurons is b, $b \\in \\mathbb{R}^{4}$. Assuming the input $x \\in \\mathbb{R}^{b*2}$, where is the batch size. Then the output of this layer is:\n",
        "$$ y = xW^T + b$$"
      ],
      "metadata": {
        "id": "qkmzheLN2Jmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**: Fill in the code to calculate the output of a layer for given input. **(1 point)**\n",
        "\n",
        "*Hints*: for matrix multiplication in pytorch, you can use `torch.matmul`. "
      ],
      "metadata": {
        "id": "29LWl7tGJA1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_layer = nn.Linear(2, 4)           # we create a layer\n",
        "print(\"transposed weight: \", a_layer.weight.T)   # print the weight of the layer\n",
        "print(\"bias: \", a_layer.bias)       # print the bias of the layer\n",
        "input = torch.rand((3, 2))          # we create a dummy input\n",
        "print('input: ', input)\n",
        "# your code here      \n",
        "output = a_layer(input)  \n",
        "print('output: ', output)          "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_9wm9qblzjw",
        "outputId": "52d17253-3d4f-43ae-ff08-058846374133"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transposed weight:  tensor([[-0.3411, -0.0891, -0.3051, -0.2172],\n",
            "        [-0.0112, -0.3751,  0.0520, -0.6078]], grad_fn=<PermuteBackward0>)\n",
            "bias:  Parameter containing:\n",
            "tensor([ 0.6034,  0.2687, -0.2343, -0.1047], requires_grad=True)\n",
            "input:  tensor([[0.0382, 0.4928],\n",
            "        [0.1588, 0.9375],\n",
            "        [0.9497, 0.1182]])\n",
            "output:  tensor([[ 0.5849,  0.0804, -0.2203, -0.4125],\n",
            "        [ 0.5387, -0.0972, -0.2340, -0.7090],\n",
            "        [ 0.2782,  0.1397, -0.5179, -0.3828]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**: We implement below architecture in the video. It works quite well. Let's try to add the third hidden layer and keep other layers. **(1 point)**"
      ],
      "metadata": {
        "id": "KC783pMOJ7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 120)  # first hidden layer\n",
        "        self.fc2 = nn.Linear(120, 84)     # second hidden layer\n",
        "        # your code here\n",
        "        self.additional_layer = ...\n",
        "        self.fc3 = nn.Linear(84, 10)      # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)   # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))   # compute output of first hidden layer + activation function\n",
        "        x = F.relu(self.fc2(x))   # compute output of second hiddlen layer + activation function\n",
        "        # your code here\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc3(x)           # compute output\n",
        "        return x                  # return output\n",
        "\n",
        "model = Net()"
      ],
      "metadata": {
        "id": "Qlc38zzsVsGZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used `CrossEntropyLoss` function to calculate loss. It is equivalent to the combination of `LogSoftmax` and `NLLLoss`. We have not talked about these two functions. They are two commonly used functions in classification problems.\n",
        "\n",
        "In the output layer, we don't use any activation function. To get the final prediction, we just need to chooose the maximum output. However, you should use softmax to get the estimated probabilities. Output of softmax function are:\n",
        "\n",
        "\n",
        "<center>$\\sigma(z)_{i} = \\frac{e^{z_{i}}}{\\sum_{j=1}^{C}e^{z_{j}}} $ for $i = 1, ..., C$ and $\\textbf{z} = \\{z_1, z_2, ..., z_C\\}$</center>\n",
        "\n",
        "\n",
        "where $C$ is number of classes, $\\textbf{z}$ is the output of network."
      ],
      "metadata": {
        "id": "t0kmvQFBl7Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3**: You are asked to implement softmax function **(1 point)**\n",
        "\n",
        "Hints: to calculate the exponential of input elements you can use *exp()* function. For example, `z.exp()`"
      ],
      "metadata": {
        "id": "raYd3kbv255I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  # your code here\n",
        "  # exponential of the input elements\n",
        "  exp = z.exp()\n",
        "  # sum the exponentials of the input elements\n",
        "  exp_sum = exp.sum(dim=1, keepdim=True)\n",
        "  # divide each exponential by the sum of all exponentials\n",
        "  output = exp / exp_sum\n",
        "  \n",
        "  return output"
      ],
      "metadata": {
        "id": "eA9waX0s2pXd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(z):\n",
        "  return softmax(z).log()"
      ],
      "metadata": {
        "id": "1pMnBoYC_77I"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4**: You are asked to implement Negative Log Likelihood as follow: **(1 point)**\n",
        "<center>$l_n = -x_n y_n$ where $\\textbf{x}$ is prediction, $\\textbf{y}$ is target in form of one-hot vector, </center> \n",
        "\n",
        "Note: in above equation $\\textbf{y}$ is an one-hot vector. Meanwhile, the below `target` is an integer value."
      ],
      "metadata": {
        "id": "cFr4H-ov7ELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nll(preds, target):\n",
        "  # your code here\n",
        "  # convert the target values to one-hot form\n",
        "  target_onehot = torch.zeros_like(preds)\n",
        "  target_onehot.scatter_(1, target.unsqueeze(1), 1.0)\n",
        "\n",
        "  # compute the NLL loss as the negative dot product of the predicted probabilities and the one-hot target vectors\n",
        "  l = -torch.matmul(preds, target_onehot.T).mean()\n",
        "\n",
        "  return l"
      ],
      "metadata": {
        "id": "zZzvfRPw7MUr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(z, target):\n",
        "  return nll(log_softmax(z), target).mean()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "z = torch.rand((3, 10))\n",
        "target = torch.tensor([0, 5, 9], dtype=torch.long)\n",
        "print(cross_entropy_loss(z, target))\n",
        "print(loss_function(z, target))\n",
        "\n",
        "# assert cross_entropy_loss(z, target) == loss_function(z, target), 'The two above task are not correct!'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_eIAavpBRPs",
        "outputId": "1a0dfc96-9d0d-4023-c02d-a25695d3e23f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3295)\n",
            "tensor(2.6039)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We constraint the model not too complex by using regularization. One way of doing that is putting constraint to weights of models. For example, in ridge regularization, we add L2 loss term to the loss function:\n",
        "$$J(\\theta) = CrossEntropyLoss(\\theta) + \\alpha \\sum\\limits_{i=1}^n\\theta_{i}^2$$\n",
        "where $\\theta$ is model's parameters (not including bias), $\\alpha$ is a hyper-paramter to control degree of regularization."
      ],
      "metadata": {
        "id": "dR_xYpSNOpNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5**: You are asked to implement loss function composed of cross entropy loss and L2 regularization loss. **(1 point)**\n",
        "\n",
        "Hints: use `model.parameters()`"
      ],
      "metadata": {
        "id": "ThudlUyGrgfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CEloss_with_l2(z, target, model, alpha):\n",
        "  CE_loss = cross_entropy_loss(z, target)\n",
        "  # Your code here\n",
        "  L2_loss=0\n",
        "  for param in model.parameters():\n",
        "    L2_loss += param.pow(2).sum()\n",
        "  L2_loss *= alpha\n",
        "  loss = CE_loss + L2_loss\n",
        "  return loss\n",
        "\n",
        "CEloss_with_l2(torch.rand(2, 10), torch.tensor([2, 3], dtype=torch.long), model, 0.001)"
      ],
      "metadata": {
        "id": "nv6SMrKOOnDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9a14f0-231b-4a26-90fa-f227f362e0c9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5977, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6**: Similarly, let implement loss function composed of cross entropy loss and L1 regularization **(1 point)**"
      ],
      "metadata": {
        "id": "mKq31_We09al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CEloss_with_l1(z, target, model, alpha):\n",
        "  # Your code here\n",
        "  # compute the cross-entropy loss for the input and target tensors\n",
        "  CE_loss = cross_entropy_loss(z, target)\n",
        "\n",
        "  # compute the L1 regularization loss for the model parameters\n",
        "  L1_loss = 0\n",
        "  for param in model.parameters():\n",
        "    L1_loss += param.abs().sum()\n",
        "  L1_loss *= alpha\n",
        "\n",
        "  # add the cross-entropy loss and the L1 regularization loss to obtain the total loss\n",
        "  loss = CE_loss + L1_loss\n",
        "  return loss\n",
        "\n",
        "CEloss_with_l1(torch.rand(2, 10), torch.tensor([2, 3], dtype=torch.long), model, 0.001)"
      ],
      "metadata": {
        "id": "npqOPHEB1KuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5efe31-a53d-48d9-d980-0afdfdee0105"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.5032, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For using L2 regularization in Pytorch, you can set `weight_decay` property of optimizer. Here, we use the default value `weight_decay=0`."
      ],
      "metadata": {
        "id": "Avtue18G2ExY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "8gNz2v3xIBLx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7:** Below code is the procedure to train and evaluate the trained model. We put more comments so you can better understand each line of code. We also put redundant lines of code in the `evaluate` section. Your task is to find all redundant code and comment them out. (**2 points**)"
      ],
      "metadata": {
        "id": "8pAgHxZJVyH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()    # move model to GPU for faster computation in parallel\n",
        "n_epochs = 2    # you should change to larger value\n",
        "for epoch in range(n_epochs):\n",
        "  print(f'Epoch {epoch}- Training... ')\n",
        "  for batch_idx, (imgs, labels) in enumerate(tqdm(train_dataloader)):\n",
        "    imgs = imgs.cuda()      # move images to GPU\n",
        "    labels = labels.cuda()  # move labels to GPU\n",
        "    optimizer.zero_grad()   # by default, Pytorch's optimizer retains gradient after apply Gradient Descent. We need to call this function to clear gradients explicitly.\n",
        "    preds = model(imgs)     # pass batch of data through the model. \n",
        "    loss = loss_function(preds, labels)   # compute loss \n",
        "    loss.backward()         # compute gradient by running backpropgation algorithm\n",
        "    optimizer.step()        # apply Gradient Descent\n",
        "  \n",
        "  # evaluate \n",
        "  print(f'Epoch {epoch}- Evaluating... ')\n",
        "  total_correct = 0\n",
        "  total = len(testset)\n",
        "  for batch_indx, (imgs, labels) in enumerate(tqdm(test_dataloader)):\n",
        "    imgs = imgs.cuda()\n",
        "    labels = labels.cuda()\n",
        "    preds = model(imgs).argmax(axis=-1)\n",
        "    # loss = loss_function(preds, labels)\n",
        "    # loss.backward()\n",
        "    n_correct = torch.sum(preds == labels)\n",
        "    total_correct += n_correct.item()\n",
        "  \n",
        "  print(\"Accuracy: \", total_correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "dY4UMUc2WTZA",
        "outputId": "6909b59e-84fc-4481-ca55-263cf9b05c41"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0- Training... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3750 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c6b54fec83f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# by default, Pytorch's optimizer retains gradient after apply Gradient Descent. We need to call this function to clear gradients explicitly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# pass batch of data through the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# compute gradient by running backpropgation algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-e8bd439f96f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m                  \u001b[0;31m# return output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x10 and 84x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 8**: In this task, you are asked to use the last trained model to make a prediction of an image on test set. You also need to calculate the confident score of that prediction using softmax function. **(2 points)**"
      ],
      "metadata": {
        "id": "nTMfYOwhQaL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = testset[0]   # you can change the index\n",
        "input_tensor = torch.tensor(img).cuda()   # convert image to tensor to be able to pass through model, and model the image to GPU\n",
        "output = model(input_tensor)\n",
        "# your code here\n",
        "value = ...\n",
        "confident_score = ...\n",
        "print(f\"Prediction: {value} - confident score: {confident_score}\")\n",
        "plt.imshow(img[0])"
      ],
      "metadata": {
        "id": "cIHAOO6Tfq9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}